# FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1%2B-red)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**[[Paper]](https://arxiv.org/pdf/2508.18389)**  **[[Website]](https://hliang2.github.io/FastAvatar/)**

<table>
  <tr>
    <td align="center"><img src="docs/assets/images/teaser.png" width="800"></td>
  </tr>
</table>

## Overview

We introduce FastAvatar, which achieves real-time 3D face reconstruction from a single image under unconstrained poses, combining high fidelity with speed.

### Key Features
- ðŸŽ¯ **Pose-invariant 3D face reconstruction** *FastAvatar takes input photo from any pose.*
- ðŸ”§ **Single-shot Gaussian Splatting** *FastAvatar requires only one input photo.*
- âš¡ **Real-time fitting** *FastAvatar is feed-forward, generalizable.*

## Quick Start

### Prerequisites
- Python 3.10 or higher
- CUDA 12.1 or higher (for GPU support)

### Installation

```bash
# Clone the repository
git clone https://github.com/hliang2/FastAvatar.git
cd FastAvatar

# Create virtual environment 
conda create --name fastavatar -y python=3.10
conda activate fastavatar

# Install PyTorch (adjust CUDA version as needed)
# Ignore the package warnings/errors at this step, will be fixed after requirements installed
pip install torch==2.1.0+cu121 torchvision==0.16.0+cu121 --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt
```

### Download Pretrained Models
All pretrained models can be downloaded **[here](https://drive.google.com/file/d/1_XPTo_1rgzxvGQcRI7Toa3iGagytPTjK/view?usp=sharing)**. 
Unzip and place them in pretrained_weights/

## Basic Usage

### Single Image Inference

Perform end-to-end inference from a single image without FLAME guidance:

```bash
# Basic single image inference
python scripts/inference_feedforward_no_guidance.py \
    --image /path/to/image.jpg \
    --encoder_checkpoint /path/to/encoder.pth \
    --decoder_checkpoint /path/to/decoder.pth \
    --dino_checkpoint /path/to/dino_model.pth
```

### Inference with FLAME guidance (nersemble-style)

```python
# Basic training with default parameters
python scripts/inference_feedforward_full_guidance.py

# Custom learning rates and regularization
python scripts/inference_feedforward_full_guidance.py \
    --sample_id 306 \
    --max_epochs 401 \
    --mlp_lr 2e-4 \
    --w_lr 1e-4 \
    --scale_reg 0.01 \
    --pos_reg 0.001

# Using different LPIPS network
python scripts/inference.py --sample_id 306 --lpips_net vgg
``` 

All experiment outputs are organized under the `results/` folder:

- **`results/images/`**  
  Contains rendered novel view images from multiple camera angles for visualization.

- **`results/ply/`**  
  Point cloud results for FLAME guidance based inference.

- **`splats.ply`**  
  Contains 3D point cloud reconstructions in `.ply` format. These can be visualized with any standard 3D viewer.  

- **`w_vector.npy`**  
  The predicted latent W vector used to generate the reconstruction.

- **`dino_points.npy`**  
  3D point predictions from the DINO model used as geometric guidance.

- **`dino_camera_pose.npy`**  
  Camera pose estimation from the DINO model (when available).

- **`summary.json`**  
  Comprehensive statistics and metadata about the reconstruction including Gaussian count, parameter statistics, and processing details.
  
You can use [Supersplat](https://superspl.at/editor) for interactive visualization of Gaussian-based point clouds.

## Training

### Decoder Training
Train the decoder on multiple subjects with COLMAP reconstructions:

```bash
# Basic decoder training
python scripts/train_decoder.py --data_root /path/to/multi_subject_data

# Custom learning rates and parameters
python scripts/train_decoder.py \
    --data_root /path/to/multi_subject_data \
    --max_epochs 800 \
    --mlp_lr 2e-4 \
    --w_lr 1e-4 \
    --base_lr 5e-5 \
    --l1_weight 0.6 \
    --ssim_weight 0.3 \
    --lpips_weight 0.1

# With regularization
python scripts/train_decoder.py \
    --data_root /path/to/multi_subject_data \
    --scale_reg 0.01 \
    --pos_reg 0.001
```

### Encoder Training
Train the encoder to predict W vectors from face embeddings:

```bash
# Basic encoder training (requires pretrained decoder)
python scripts/train_encoder.py \
    --data_root /path/to/multi_subject_data \
    --decoder_load_path /path/to/decoder.pth

# Custom training parameters
python scripts/train_encoder.py \
    --data_root /path/to/multi_subject_data \
    --decoder_load_path /path/to/decoder.pth \
    --encoder_lr 2e-4 \
    --batch_size 128 \
    --max_epochs 800 \
    --mse_weight 0.2 \
    --cosine_weight 0.8

# With contrastive learning
python scripts/train_encoder.py \
    --data_root /path/to/multi_subject_data \
    --decoder_load_path /path/to/decoder.pth \
    --contrastive_weight 0.1 \
    --contrastive_temperature 0.07
```

### Training Data Structure
For training multiple subjects, organize data as **[VHAP](https://github.com/ShenhanQian/VHAP)** style:

```bash
training_data/
â”œâ”€â”€ 001_EXP-1_v16_DS4_whiteBg_staticOffset_maskBelowLine/
â”‚   â”œâ”€â”€ images/           # Face images for subject 001
â”‚   â””â”€â”€ sparse/           # COLMAP reconstruction
â”œâ”€â”€ 002_EXP-1_v16_DS4_whiteBg_staticOffset_maskBelowLine/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ sparse/
â””â”€â”€ ...
```


## Acknowledgments

This project builds upon several excellent open-source projects:

- **[gsplat](https://github.com/nerfstudio-project/gsplat)** - Efficient CUDA kernels for differentiable Gaussian rasterization from Nerfstudio
- **[InsightFace](https://github.com/deepinsight/insightface)** - State-of-the-art face analysis toolkit for face recognition and embedding
- **[VHAP](https://github.com/ShenhanQian/VHAP)** - Head Tracking with FLAME
- **[DINOv2](https://github.com/facebookresearch/dinov2)** - Image features tracking

## Citation

If you find this work useful for your research, please consider citing:

```bibtex
@article{liang2025fastavatar,
  title={FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses},
  author={Liang, Hao and Ge, Zhixuan and Tiwari, Ashish and Majee, Soumendu and Godaliyadda, GM and Veeraraghavan, Ashok and Balakrishnan, Guha},
  journal={arXiv preprint arXiv:2508.18389},
  year={2025}
}
```


## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


**Note**: This project is for research purposes. Please ensure you have appropriate rights and consent when using face data.